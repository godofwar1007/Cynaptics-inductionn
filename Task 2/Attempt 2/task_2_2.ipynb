{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOoBw+oMaTgO7hoYDae3S+6",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/godofwar1007/Cynaptics-inductionn/blob/main/task_2_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Important libraries"
      ],
      "metadata": {
        "id": "uZC-e-v9xHob"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PrBMKJsXvXih"
      },
      "outputs": [],
      "source": [
        "\n",
        "!pip install librosa\n",
        "!pip install kaggle\n",
        "from google.colab import files\n",
        "files.upload()\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "import torch.optim as optim\n",
        "import torchvision.utils as vutils\n",
        "\n",
        "import pandas as pd\n",
        "import librosa\n",
        "import numpy as np\n",
        "import os\n",
        "import time\n",
        "from pathlib import Path"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "some helper thingies"
      ],
      "metadata": {
        "id": "vmMertAfxNDc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir -p ~/.kaggle\n",
        "!mv kaggle.json ~/.kaggle/\n",
        "!chmod 600 ~/.kaggle/kaggle.json\n",
        "\n",
        "!kaggle competitions download -c the-frequency-quest"
      ],
      "metadata": {
        "id": "J_vvs1CfxPub"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip the-frequency-quest.zip -d ./data"
      ],
      "metadata": {
        "id": "9RGgFZ7GxSCP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_default_device():\n",
        "    if torch.cuda.is_available():\n",
        "        return torch.device('cuda')\n",
        "    else:\n",
        "        return torch.device('cpu')\n",
        "\n",
        "def to_device(data, device):\n",
        "    if isinstance(data, (list, tuple)):\n",
        "        return [to_device(x, device) for x in data]\n",
        "    return data.to(device, non_blocking=True)\n",
        "\n",
        "class DeviceDataLoader():\n",
        "    def __init__(self, dl, device):\n",
        "        self.dl = dl\n",
        "        self.device = device\n",
        "\n",
        "    def __iter__(self):\n",
        "        for b in self.dl:\n",
        "            yield to_device(b, self.device)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dl)\n"
      ],
      "metadata": {
        "id": "SZ-oy_WsxTx7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The audio dataset"
      ],
      "metadata": {
        "id": "XwZ7YJ1zxZV4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class AudioDataset(Dataset):\n",
        "    def __init__(self, df, n_mels=128, fixed_width=300):\n",
        "        self.df = df\n",
        "        self.n_mels = n_mels\n",
        "        self.fixed_width = fixed_width\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        row = self.df.iloc[index]\n",
        "        filepath = row['filepath']\n",
        "\n",
        "        try:\n",
        "            y, sr = librosa.load(filepath, sr=22050)\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading {filepath}: {e}\")\n",
        "            return torch.zeros((3, self.n_mels, self.fixed_width))\n",
        "\n",
        "        S = librosa.feature.melspectrogram(y=y, sr=sr, n_mels=self.n_mels)\n",
        "        S_db = librosa.power_to_db(S, ref=np.max) # S_db is in range [~-80, 0]\n",
        "\n",
        "        if S_db.shape[1] > self.fixed_width:\n",
        "            S_db = S_db[:, :self.fixed_width]\n",
        "        else:\n",
        "            pad_width = self.fixed_width - S_db.shape[1]\n",
        "            S_db = np.pad(S_db, ((0, 0), (0, pad_width)), mode='constant')\n",
        "\n",
        "        S_db_3channel = np.stack([S_db, S_db, S_db], axis=0)\n",
        "\n",
        "        normalized_spec = (S_db_3channel + 40.0) / 40.0\n",
        "\n",
        "        normalized_spec = np.clip(normalized_spec, -1.0, 1.0)\n",
        "\n",
        "        return torch.tensor(normalized_spec, dtype=torch.float32)"
      ],
      "metadata": {
        "id": "wehBkkvIxZ68"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The dicriminator"
      ],
      "metadata": {
        "id": "KFZ1OguwxdoY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def conv_block(in_channels, out_channels, pool=False):\n",
        "    layers = [nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n",
        "              nn.BatchNorm2d(out_channels),\n",
        "              nn.LeakyReLU(0.2, inplace=True)]\n",
        "    if pool: layers.append(nn.MaxPool2d(2))\n",
        "    return nn.Sequential(*layers)\n",
        "\n",
        "class Discriminator(nn.Module):\n",
        "\n",
        "    def __init__(self, in_channels=3):\n",
        "        super().__init__()\n",
        "\n",
        "        # Input: 3 x 128 x 300\n",
        "        self.model = nn.Sequential(\n",
        "            conv_block(in_channels, 64, pool=True), # -> 64 x 64 x 150\n",
        "            conv_block(64, 128, pool=True),         # -> 128 x 32 x 75\n",
        "            conv_block(128, 256, pool=True),        # -> 256 x 16 x 37\n",
        "\n",
        "            nn.AdaptiveAvgPool2d((1, 1)), # -> 256 x 1 x 1\n",
        "            nn.Flatten(),                 # -> 256\n",
        "            nn.Linear(256, 1),            # -> 1 (Real/Fake)\n",
        "            nn.Sigmoid()                  # -> 0-1 probability\n",
        "        )\n",
        "\n",
        "    def forward(self, xb):\n",
        "        return self.model(xb)"
      ],
      "metadata": {
        "id": "2k2sq8MaximF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "THe generator"
      ],
      "metadata": {
        "id": "mCvdoaEjxs7z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Generator(nn.Module):\n",
        "    def __init__(self, latent_size=100, ngf=128):\n",
        "        super().__init__()\n",
        "        self.latent_size = latent_size\n",
        "\n",
        "        self.model = nn.Sequential(\n",
        "            nn.Linear(latent_size, ngf * 8 * 8 * 19), # 128 * 8 * 19 = 19456\n",
        "            nn.BatchNorm1d(ngf * 8 * 8 * 19),\n",
        "            nn.ReLU(),\n",
        "\n",
        "            nn.Unflatten(1, (ngf * 8, 8, 19)),\n",
        "\n",
        "            nn.ConvTranspose2d(ngf * 8, ngf * 4, kernel_size=4, stride=2, padding=1, bias=False), # -> [512, 16, 38]\n",
        "            nn.BatchNorm2d(ngf * 4),\n",
        "            nn.ReLU(True),\n",
        "\n",
        "            # State: [512, 16, 38]\n",
        "            nn.ConvTranspose2d(ngf * 4, ngf * 2, kernel_size=4, stride=2, padding=1, bias=False), # -> [256, 32, 76]\n",
        "            nn.BatchNorm2d(ngf * 2),\n",
        "            nn.ReLU(True),\n",
        "\n",
        "            # State: [256, 32, 76]\n",
        "            nn.ConvTranspose2d(ngf * 2, ngf, kernel_size=4, stride=2, padding=1, bias=False), # -> [128, 64, 152]\n",
        "            nn.BatchNorm2d(ngf),\n",
        "            nn.ReLU(True),\n",
        "\n",
        "            # State: [128, 64, 152]\n",
        "            nn.ConvTranspose2d(ngf, 3, kernel_size=4, stride=2, padding=1, bias=False), # -> [3, 128, 304]\n",
        "\n",
        "            # must crop to the exact size of [3, 128, 300]\n",
        "            nn.AdaptiveAvgPool2d((128, 300)),\n",
        "\n",
        "            # Output normalized to [-1, 1]\n",
        "            nn.Tanh()\n",
        "        )\n",
        "\n",
        "    def forward(self, z):\n",
        "        return self.model(z)"
      ],
      "metadata": {
        "id": "Gngd_duFxt7R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Running the model"
      ],
      "metadata": {
        "id": "uGrJYQrvyMH0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!ls -l ./data"
      ],
      "metadata": {
        "id": "tZ7yjORXyMvT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls -l ./data/train"
      ],
      "metadata": {
        "id": "PFHJMPLLyP-t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"=\"*30)\n",
        "print(\"Starting Audio GAN Training...\")\n",
        "\n",
        "device = get_default_device()\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "os.makedirs(\"gan_audio_images\", exist_ok=True)\n",
        "\n",
        "N_MELS = 128\n",
        "FIXED_WIDTH = 300\n",
        "BATCH_SIZE = 32\n",
        "lr = 0.0002\n",
        "num_epochs = 50\n",
        "latent_size = 100\n",
        "\n",
        "print(\"Scanning audio files...\")\n",
        "data_dir = Path(\"./data\")\n",
        "train_audio_path = data_dir / \"train\" / \"train\" # Corrected path\n",
        "filepaths = []\n",
        "\n",
        "for folder in train_audio_path.iterdir():\n",
        "    if folder.is_dir():\n",
        "\n",
        "        for ext in ['*.wav', '*.mp3', '*.ogg']:\n",
        "            for file in folder.glob(ext):\n",
        "                filepaths.append(file)\n",
        "df = pd.DataFrame({'filepath': filepaths})\n",
        "train_ds = AudioDataset(df, n_mels=N_MELS, fixed_width=FIXED_WIDTH)\n",
        "train_dl = DataLoader(train_ds, BATCH_SIZE, shuffle=True, num_workers=2, pin_memory=True)\n",
        "train_dl = DeviceDataLoader(train_dl, device)\n",
        "print(f\"DataLoaders are ready. Found {len(df)} real audio files.\")\n",
        "\n",
        "D = Discriminator(in_channels=3).to(device)\n",
        "G = Generator(latent_size=latent_size).to(device)\n",
        "\n",
        "criterion = nn.BCELoss()\n",
        "\n",
        "\"\"\" apparently the D was too smart and the d_loss was very low during a training and\n",
        "gan was collapsing so i fixed it by nerfing the D by reducing its lr by 10\"\"\"\n",
        "d_lr = lr\n",
        "g_lr = lr\n",
        "\n",
        "print(f\"Starting training with D_lr: {d_lr} and G_lr: {g_lr}\")\n",
        "d_optimizer = optim.Adam(D.parameters(), lr=d_lr, betas=(0.5, 0.999))\n",
        "g_optimizer = optim.Adam(G.parameters(), lr=g_lr, betas=(0.5, 0.999))\n",
        "\n",
        "fixed_noise = torch.randn(BATCH_SIZE, latent_size).to(device)"
      ],
      "metadata": {
        "id": "ieUsFkkfyRjX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training loop"
      ],
      "metadata": {
        "id": "FnCT7wo1yiYH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Starting GAN training! This will take a while...\")\n",
        "total_step = len(train_dl)\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    for i, real_images in enumerate(train_dl):\n",
        "\n",
        "        current_batch_size = real_images.size(0)\n",
        "\n",
        "        D.zero_grad()\n",
        "        real_labels = torch.ones(current_batch_size, 1).to(device)\n",
        "\n",
        "        outputs = D(real_images)\n",
        "        d_loss_real = criterion(outputs, real_labels)\n",
        "\n",
        "        z = torch.randn(current_batch_size, latent_size).to(device)\n",
        "        fake_images = G(z)\n",
        "        fake_labels = torch.zeros(current_batch_size, 1).to(device)\n",
        "\n",
        "        outputs = D(fake_images.detach())\n",
        "        d_loss_fake = criterion(outputs, fake_labels)\n",
        "\n",
        "        d_loss = d_loss_real + d_loss_fake\n",
        "        d_loss.backward()\n",
        "        d_optimizer.step()\n",
        "\n",
        "        G.zero_grad()\n",
        "\n",
        "        outputs = D(fake_images)\n",
        "        g_loss = criterion(outputs, real_labels)\n",
        "\n",
        "        g_loss.backward()\n",
        "        g_optimizer.step()\n",
        "\n",
        "        if (i+1) % 100 == 0:\n",
        "            print(f\"Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{total_step}], \"\n",
        "                  f\"D_loss: {d_loss.item():.4f}, G_loss: {g_loss.item():.4f}\")\n",
        "\n",
        "    G.eval()\n",
        "    with torch.no_grad():\n",
        "        fake_images_fixed = G(fixed_noise)\n",
        "\n",
        "        fake_images_fixed = (fake_images_fixed + 1) / 2\n",
        "\n",
        "        vutils.save_image(fake_images_fixed, f\"gan_audio_images/epoch_{epoch+1}.png\", normalize=True)\n",
        "    G.train()\n",
        "\n",
        "print(\"TRAINING FINISHED!\")\n",
        "\n",
        "torch.save(G.state_dict(), 'audio_generator_model.pth')\n",
        "print(\"Generator model saved to audio_generator_model.pth\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sgquNju2ypMa",
        "outputId": "3ace0456-a921-4003-aecf-993da9197349"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Starting GAN training! This will take a while...\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
            "  warnings.warn(warn_msg)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [1/50], Step [100/108], D_loss: 0.4228, G_loss: 2.0572\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Generating audio"
      ],
      "metadata": {
        "id": "pZ4geZOtyxa1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "!pip install soundfile\n",
        "import soundfile as sf\n",
        "\n",
        "print(\"=\"*30)\n",
        "print(\"Generating final audio samples...\")\n",
        "\n",
        "device = get_default_device()\n",
        "latent_size = 100\n",
        "model_path = 'audio_generator_model.pth'\n",
        "G = Generator(latent_size=latent_size).to(device)\n",
        "G.load_state_dict(torch.load(model_path))\n",
        "G.eval()\n",
        "print(\"Generator model loaded.\")\n",
        "\n",
        "os.makedirs(\"gan_audio_files\", exist_ok=True)\n",
        "\n",
        "\n",
        "num_samples = 5\n",
        "sample_rate = 22050\n",
        "\n",
        "with torch.no_grad():\n",
        "    for i in range(num_samples):\n",
        "\n",
        "        z = torch.randn(1, latent_size).to(device)\n",
        "        fake_spec_tensor = G(z)\n",
        "        fake_spec_norm = fake_spec_tensor.cpu().numpy()[0]\n",
        "        fake_spec_1ch = fake_spec_norm[0, :, :]\n",
        "\n",
        "        fake_spec_db = (fake_spec_1ch * 40.0) - 40.0\n",
        "\n",
        "        S_power = librosa.db_to_power(fake_spec_db)\n",
        "\n",
        "        y_fake = librosa.feature.inverse.mel_to_audio(\n",
        "            S_power,\n",
        "            sr=sample_rate,\n",
        "            n_fft=2048,\n",
        "            hop_length=512\n",
        "        )\n",
        "\n",
        "        filename = f\"gan_audio_files/generated_audio_{i+1}.wav\"\n",
        "        sf.write(filename, y_fake, sample_rate)\n",
        "\n",
        "print(f\"Successfully generated {num_samples} audio files in 'gan_audio_files' folder!\")\n",
        "print(\"=\"*30)"
      ],
      "metadata": {
        "id": "t_BgM_57y1SA"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
